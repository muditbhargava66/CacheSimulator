# v1.2.0 Feature Guide

This guide covers all the new features introduced in Cache Simulator v1.2.0.

## Table of Contents
1. [NRU Replacement Policy](#nru-replacement-policy)
2. [Victim Cache](#victim-cache)
3. [Write Policies](#write-policies)
4. [Parallel Processing](#parallel-processing)
5. [Multi-Processor Simulation](#multi-processor-simulation)
6. [Statistical Visualization](#statistical-visualization)
7. [Performance Analysis Tools](#performance-analysis-tools)

## NRU Replacement Policy

The Not Recently Used (NRU) replacement policy is a cost-effective approximation of LRU that uses a single reference bit per cache block.

### How It Works
- Each cache block has a reference bit
- On access, the reference bit is set to 1
- On replacement, prefer blocks with reference bit = 0
- Periodically clear all reference bits

### Configuration
```json
{
  "l1": {
    "replacementPolicy": "NRU"
  }
}
```

### When to Use
- Large caches where LRU overhead is significant
- Workloads with clear hot/cold data separation
- Systems with limited metadata storage

### Example
```bash
# Run with NRU policy
./cachesim --replacement-policy NRU traces/workload.txt
```

## Victim Cache

A small, fully-associative cache that stores recently evicted blocks from the main cache.

### Benefits
- Reduces conflict misses by 15-25%
- Particularly effective for direct-mapped or low-associativity caches
- Minimal hardware overhead (typically 4-16 entries)

### Configuration
```json
{
  "victimCache": {
    "enabled": true,
    "size": 8,
    "replacementPolicy": "FIFO"
  }
}
```

### How It Works
1. On cache miss and eviction, the evicted block goes to victim cache
2. On cache miss, check victim cache before going to next level
3. If found in victim cache, swap with a block in main cache

### Example Usage
```bash
# Enable 8-entry victim cache
./cachesim --victim-cache --victim-size 8 traces/conflict_heavy.txt
```

### Best Practices
- Use with small or low-associativity L1 caches
- Size between 4-16 entries is typically optimal
- Monitor victim cache hit rate to determine effectiveness

## Write Policies

### No-Write-Allocate
Prevents allocation of cache lines on write misses, reducing cache pollution for write-only data.

```json
{
  "l1": {
    "writePolicy": "WriteThrough",
    "writeAllocate": false
  }
}
```

### Write Combining Buffer
Coalesces multiple writes to the same cache line before writing to memory.

```json
{
  "writeCombining": {
    "enabled": true,
    "bufferSize": 8,
    "timeout": 100
  }
}
```

### When to Use Each Policy

| Policy | Best For | Avoid When |
|--------|----------|------------|
| Write-Back + Allocate | General purpose | Write-once data |
| Write-Through + No-Allocate | Streaming writes | High reuse data |
| Write Combining | Burst writes | Random writes |

## Parallel Processing

### Thread Pool Execution
```cpp
// Enable parallel trace processing
ParallelTraceProcessor<CacheSimulator> processor(8); // 8 threads
auto result = processor.processTrace(traceFile, config);
```

### Command Line Usage
```bash
# Use 8 threads for simulation
./cachesim -p 8 traces/large_trace.txt

# Auto-detect CPU cores
./cachesim -p auto traces/large_trace.txt
```

### Performance Scaling
- Near-linear speedup up to 4 threads
- 3.5-4x speedup on 8-core systems
- Best for traces > 1M accesses

## Multi-Processor Simulation

### MESI Protocol States
- **Modified (M)**: Exclusive ownership, dirty
- **Exclusive (E)**: Exclusive ownership, clean  
- **Shared (S)**: Shared with other caches
- **Invalid (I)**: Not present or invalid

### Configuration
```json
{
  "multiprocessor": {
    "numCores": 4,
    "coherence": "MESI",
    "interconnect": "Bus",
    "interconnectLatency": 10,
    "privateL1": true,
    "sharedL2": true
  }
}
```

### Trace Format for Multi-Processor
```
# Format: <op> <address> [processor_id]
r 0x1000 0   # Processor 0 reads
w 0x1000 1   # Processor 1 writes (invalidates P0's copy)
r 0x1000 2   # Processor 2 reads (downgrades P1 to Shared)
```

### Coherence Traffic Analysis
```bash
# Run multi-processor simulation
./cachesim --mp-config mp_config.json traces/parallel_workload.txt

# Output includes:
# - Coherence messages per processor
# - State transition statistics
# - False sharing detection
```

## Statistical Visualization

### Available Chart Types

#### Line Charts
```cpp
auto data = getHitRateOverTime();
std::cout << Visualization::generateLineChart(
    data, 80, 20,
    "Hit Rate Trend",
    "Time (K accesses)",
    "Hit Rate (%)"
);
```

#### Pie Charts
```cpp
std::vector<std::pair<std::string, double>> missTypes = {
    {"Compulsory", 15.0},
    {"Capacity", 35.0},
    {"Conflict", 40.0},
    {"Coherence", 10.0}
};
std::cout << Visualization::generatePieChart(missTypes, 15);
```

#### Scatter Plots
```cpp
auto cacheVsPerf = getCacheSizeVsPerformance();
std::cout << Visualization::generateScatterPlot(
    cacheVsPerf, 60, 20,
    "Cache Size vs Performance"
);
```

### Command Line Usage
```bash
# Enable all visualizations
./cachesim --charts traces/workload.txt

# Export charts to CSV
./cachesim --charts --export-charts results.csv traces/workload.txt
```

## Performance Analysis Tools

### Cache Analyzer
Comprehensive trace analysis including:
- Working set size analysis
- Reuse distance calculation
- Access pattern classification
- Optimal cache configuration recommendations

```bash
# Full analysis with visualizations
./tools/cache_analyzer -v -g -o analysis.csv traces/workload.txt

# Quick analysis
./tools/cache_analyzer traces/workload.txt
```

### Performance Comparison
Compare multiple cache configurations:

```bash
# Compare default configurations
./tools/performance_comparison traces/workload.txt

# Add custom configurations
./tools/performance_comparison -c config1.json -c config2.json traces/workload.txt

# Generate comparison charts
./tools/performance_comparison -g -r traces/workload.txt
```

### Output Example
```
=== Performance Comparison Results ===
Configuration          L1 Hit%   L2 Hit%   Overall%   Avg Time
----------------------------------------------------------------
NRU + Victim Cache      87.8      79.1      97.5       3.5
Basic LRU               85.2      78.3      96.7       4.8
Write-Through NA        84.9      77.8      96.3       5.2
```

## Best Practices for v1.2.0 Features

### 1. Choosing Replacement Policies
- **LRU**: Best for small caches with regular access patterns
- **NRU**: Good for large caches, lower overhead than LRU
- **FIFO**: Simple workloads, predictable behavior
- **Random**: Avoid pathological cases

### 2. Victim Cache Sizing
```bash
# Test different victim cache sizes
for size in 4 8 12 16; do
    ./cachesim --victim-cache --victim-size $size traces/workload.txt
done
```

### 3. Write Policy Selection
- Analyze write ratio: `./tools/cache_analyzer traces/workload.txt`
- High write ratio (>30%): Consider write-back
- Streaming writes: Use no-write-allocate
- Burst writes: Enable write combining

### 4. Parallel Processing
- Traces < 100K accesses: Single-threaded is fine
- Traces > 1M accesses: Use `-p auto`
- Memory-bound workloads: Limit to physical cores

### 5. Multi-Processor Optimization
- Pad data structures to avoid false sharing
- Use appropriate interconnect for core count:
  - 2-4 cores: Bus
  - 4-8 cores: Crossbar  
  - 8+ cores: Mesh

## Troubleshooting

### Common Issues

**Q: Victim cache shows low hit rate**
- Check if L1 associativity is already high
- Verify conflict misses with cache analyzer
- Try different victim cache sizes

**Q: Parallel simulation slower than sequential**
- Check trace size (needs >100K accesses)
- Verify thread count matches CPU cores
- Check for I/O bottlenecks

**Q: High coherence traffic in multi-processor**
- Look for false sharing patterns
- Check data structure alignment
- Consider private vs shared caches

## Example Workflows

### Optimizing for Conflict Misses
```bash
# 1. Analyze trace for conflict patterns
./tools/cache_analyzer -v traces/workload.txt

# 2. Test with victim cache
./cachesim --victim-cache traces/workload.txt

# 3. Compare different sizes
./tools/performance_comparison --victim-study traces/workload.txt
```

### Multi-Core Performance Analysis
```bash
# 1. Generate multi-processor trace
./tools/trace_generator -p multicore -n 100000 -o mp_trace.txt

# 2. Run with coherence analysis
./cachesim --mp-cores 4 --coherence-stats mp_trace.txt

# 3. Visualize coherence traffic
./cachesim --mp-cores 4 --charts --coherence-viz mp_trace.txt
```

## Performance Tuning Checklist

- [ ] Profile workload with cache analyzer
- [ ] Select appropriate replacement policy
- [ ] Size victim cache based on conflict misses
- [ ] Choose write policy based on write patterns
- [ ] Enable parallel processing for large traces
- [ ] Configure coherence protocol for multi-processor
- [ ] Use visualization to identify bottlenecks
- [ ] Compare configurations with performance tool
- [ ] Export results for further analysis
- [ ] Document optimal configuration
